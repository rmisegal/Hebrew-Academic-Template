% Basic bibliography file for Hebrew Academic Template examples
% Contains 5-10 essential references for basic demonstrations

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017},
  keywords={english}
}

@article{devlin2018bert,
  title={BERT: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018},
  keywords={english}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020},
  keywords={english}
}

@article{hebrew_nlp_2023,
  title={עיבוד שפה טבעית בעברית: אתגרים ופתרונות},
  author={כהן, דוד and לוי, שרה},
  journal={כתב עת לבלשנות חישובית},
  volume={15},
  number={3},
  pages={234--256},
  year={2023},
  keywords={hebrew}
}

@book{hebrew_linguistics_2022,
  title={בלשנות עברית מודרנית},
  author={ישראלי, משה and כהן, רחל},
  publisher={הוצאת האוניברסיטה העברית},
  year={2022},
  address={ירושלים},
  keywords={hebrew}
}

@article{mikolov2013,
  title={Efficient estimation of word representations in vector space},
  author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  journal={arXiv preprint arXiv:1301.3781},
  year={2013},
  keywords={english}
}

@inproceedings{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  booktitle={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019},
  keywords={english}
}

@article{bert_paper_2018,
  title={BERT: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={Proceedings of NAACL-HLT},
  pages={4171--4186},
  year={2019},
  keywords={english}
}

@techreport{gpt3_paper_2020,
  title={GPT-3: Language models are few-shot learners},
  author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  institution={OpenAI},
  year={2020},
  keywords={english}
}

@article{hebrew_computational_2021,
  title={אתגרים חישוביים בעיבוד טקסט עברי},
  author={אברהם, יוסף and שמעון, לאה},
  journal={מחקרי מחשב ושפה},
  volume={8},
  number={2},
  pages={112--128},
  year={2021},
  keywords={hebrew}
}